{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "903cf788-981e-49a3-ac79-3dc8a66802ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/23 06:12:48 WARN Utils: Your hostname, bartosz resolves to a loopback address: 127.0.1.1; using 192.168.1.55 instead (on interface wlp0s20f3)\n",
      "24/11/23 06:12:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/bartosz/.venvs/pyspark-delta-lake-jupyterlab/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/bartosz/.ivy2/cache\n",
      "The jars for the packages stored in: /home/bartosz/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-36432dcc-e834-44c2-948b-62a2a7d870bd;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.0.0 in central\n",
      "\tfound io.delta#delta-storage;3.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in local-m2-cache\n",
      ":: resolution report :: resolve 373ms :: artifacts dl 18ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from local-m2-cache in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-36432dcc-e834-44c2-948b-62a2a7d870bd\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/10ms)\n",
      "24/11/23 06:12:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from delta import configure_spark_with_delta_pip, DeltaTable\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_session = (configure_spark_with_delta_pip(SparkSession.builder.master(\"local[*]\")\n",
    "                                                        .config(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "                                                .config(\"spark.sql.extensions\",\n",
    "                                                        \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "                                                .config(\"spark.sql.catalog.spark_catalog\",\n",
    "                                                        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "                                                ).getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71880432-077b-4bed-b499-6ef8f8cd5bcc",
   "metadata": {},
   "source": [
    "**Create input tables first**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0622a7c3-7c7a-4fff-87eb-7723ea58f063",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf ./spark-warehouse && rm -rf ./metastore_db/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faec8d86-aa35-45ae-b055-0094f6406dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/23 06:12:56 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/11/23 06:12:56 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "24/11/23 06:13:00 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "24/11/23 06:13:00 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore bartosz@127.0.1.1\n",
      "24/11/23 06:13:00 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException\n",
      "24/11/23 06:13:05 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "24/11/23 06:13:10 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`default`.`versions` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "24/11/23 06:13:10 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "24/11/23 06:13:10 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "24/11/23 06:13:10 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/11/23 06:13:10 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "24/11/23 06:13:13 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`default`.`numbers_letters` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_session.sql('''\n",
    "          CREATE TABLE IF NOT EXISTS `default`.`versions` (\n",
    "            job_version STRING NOT NULL,\n",
    "            delta_table_version INT NOT NULL\n",
    "          ) USING DELTA\n",
    "        ''')\n",
    "\n",
    "spark_session.sql('''\n",
    "          CREATE TABLE IF NOT EXISTS `default`.`numbers_letters` (\n",
    "            number INT NOT NULL,\n",
    "            letter STRING NOT NULL\n",
    "          ) USING DELTA\n",
    "        ''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984a15e0-f2cd-4b42-ac07-7994d7b04c1d",
   "metadata": {},
   "source": [
    "Let's simulate the writing operations. Whenever we write, thus `MERGE` rows to the table, we update the state table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9a7d3f3-4720-4287-8609-8b73d3d4259d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/23 06:13:20 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark import Row\n",
    "\n",
    "def insert_last_version_for_the_job(job_id: str):\n",
    "    last_written_version_after_data_insert = (spark_session.sql('DESCRIBE HISTORY default.numbers_letters')\n",
    "                            .selectExpr('MAX(version) AS last_version').collect()[0].last_version)\n",
    "    new_version = (spark_session\n",
    "                   .createDataFrame([Row(job_version=job_id, delta_table_version=last_written_version_after_data_insert)]))\n",
    "    (DeltaTable.forName(spark_session, 'versions').alias('old_versions')\n",
    "     .merge(new_version.alias('new_version'), 'old_versions.job_version = new_version.job_version')\n",
    "     .whenMatchedUpdateAll().whenNotMatchedInsertAll().execute())\n",
    "def merge_tables(new_numbers_letters):\n",
    "    # hack as the DeltaTable doesn't support userMetadata settings at the operation level\n",
    "    spark_session.conf.set(\"spark.databricks.delta.commitInfo.userMetadata\", \";origin=merge;\")\n",
    "    (DeltaTable.forName(spark_session, 'numbers_letters').alias('base_table')\n",
    "     .merge(new_numbers_letters.alias('new_table'), 'base_table.number = new_table.number')\n",
    "     .whenMatchedUpdateAll().whenNotMatchedInsertAll().execute())\n",
    "\n",
    "job_id = '2024-10-01'\n",
    "merge_tables(spark_session.createDataFrame([Row(number=1, letter='a'), Row(number=2, letter='b'), Row(number=3, letter='c')]))\n",
    "insert_last_version_for_the_job(job_id)\n",
    "\n",
    "job_id = '2024-10-02'\n",
    "merge_tables(spark_session.createDataFrame([Row(number=1, letter='A'), Row(number=4, letter='d')]))\n",
    "insert_last_version_for_the_job(job_id)\n",
    "\n",
    "job_id = '2024-10-03'\n",
    "merge_tables(spark_session.createDataFrame([Row(number=1, letter='AA'), Row(number=5, letter='e'), Row(number=6, letter='f')]))\n",
    "insert_last_version_for_the_job(job_id)\n",
    "\n",
    "job_id = '2024-10-04'\n",
    "merge_tables(spark_session.createDataFrame([Row(number=1, letter='AAA'), Row(number=7, letter='g'), Row(number=8, letter='h')]))\n",
    "insert_last_version_for_the_job(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b0b8b4-b381-40cb-bf77-afc90d8bad76",
   "metadata": {},
   "source": [
    "Before we merge new datasets, turns out we have to run an `OPTIMIZE` command that is outside the data orchestration layer, so it won't create a new version to the state table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "844bca95-eb33-456e-baf1-74f19e9913e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_session.conf.unset('spark.databricks.delta.commitInfo.userMetadata')\n",
    "DeltaTable.forName(spark_session, 'numbers_letters').optimize().executeZOrderBy('number')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c73be97-fe4d-4efc-b2e4-49666dee8337",
   "metadata": {},
   "source": [
    "Let's see if the table's history has now an operation different than the `MERGE`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4ac3cce-78e8-44e0-851c-3d0a39abb0ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------------+--------------+\n",
      "|version|timestamp              |operation   |userMetadata  |\n",
      "+-------+-----------------------+------------+--------------+\n",
      "|5      |2024-11-23 06:13:44.425|OPTIMIZE    |NULL          |\n",
      "|4      |2024-11-23 06:13:38.787|MERGE       |;origin=merge;|\n",
      "|3      |2024-11-23 06:13:32.553|MERGE       |;origin=merge;|\n",
      "|2      |2024-11-23 06:13:26.562|MERGE       |;origin=merge;|\n",
      "|1      |2024-11-23 06:13:17.858|MERGE       |;origin=merge;|\n",
      "|0      |2024-11-23 06:13:11.171|CREATE TABLE|NULL          |\n",
      "+-------+-----------------------+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_session.sql('DESCRIBE HISTORY default.numbers_letters').select('version', 'timestamp', 'operation', 'userMetadata').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ae77e9-f479-42b5-97ed-f85c374c5cc1",
   "metadata": {},
   "source": [
    "Let's run a `SELECT` on the numbers_letters table to see if there is no entry corresponding to the `OPTIMIZE`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec8292ed-048e-4f8a-8882-a16ffb002ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+\n",
      "|job_version|delta_table_version|\n",
      "+-----------+-------------------+\n",
      "|2024-10-04 |4                  |\n",
      "|2024-10-03 |3                  |\n",
      "|2024-10-02 |2                  |\n",
      "|2024-10-01 |1                  |\n",
      "+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_session.sql('SELECT * FROM default.versions ORDER BY job_version DESC').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a83f3f-8232-4000-ad3e-7e4ffc414ded",
   "metadata": {},
   "source": [
    "OK, let's run now two other `MERGE` commands..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37de6bcc-ba26-47b9-9be8-1dc81bcbc18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "job_id = '2024-10-05'\n",
    "merge_tables(spark_session.createDataFrame([Row(number=1, letter='AAAA'), Row(number=9, letter='i')]))\n",
    "insert_last_version_for_the_job(job_id)\n",
    "\n",
    "job_id = '2024-10-06'\n",
    "merge_tables(spark_session.createDataFrame([Row(number=10, letter='j')]))\n",
    "insert_last_version_for_the_job(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b4f6ce-ce4d-4a02-8b49-d37d63d86abc",
   "metadata": {},
   "source": [
    "**It's time to restore the table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1de0dcac-0f6f-427c-ab1f-494e1f4c32d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------------+--------------+\n",
      "|version|timestamp              |operation   |userMetadata  |\n",
      "+-------+-----------------------+------------+--------------+\n",
      "|7      |2024-11-23 06:13:52.49 |MERGE       |;origin=merge;|\n",
      "|6      |2024-11-23 06:13:48.142|MERGE       |;origin=merge;|\n",
      "|5      |2024-11-23 06:13:44.425|OPTIMIZE    |NULL          |\n",
      "|4      |2024-11-23 06:13:38.787|MERGE       |;origin=merge;|\n",
      "|3      |2024-11-23 06:13:32.553|MERGE       |;origin=merge;|\n",
      "|2      |2024-11-23 06:13:26.562|MERGE       |;origin=merge;|\n",
      "|1      |2024-11-23 06:13:17.858|MERGE       |;origin=merge;|\n",
      "|0      |2024-11-23 06:13:11.171|CREATE TABLE|NULL          |\n",
      "+-------+-----------------------+------------+--------------+\n",
      "\n",
      "+-----------+-------------------+\n",
      "|job_version|delta_table_version|\n",
      "+-----------+-------------------+\n",
      "|2024-10-01 |1                  |\n",
      "|2024-10-02 |2                  |\n",
      "|2024-10-03 |3                  |\n",
      "|2024-10-04 |4                  |\n",
      "|2024-10-05 |6                  |\n",
      "|2024-10-06 |7                  |\n",
      "+-----------+-------------------+\n",
      "\n",
      "=============== # FIRST MERGE / 2024-10-01 ===============\n",
      "Versions for each run are: {'2024-10-01': 1}\n",
      "Last merge version was 7\n",
      "Missing previous version, truncating the table\n",
      "Version to restore is None\n",
      "Expected: TRUNCATE TABLE\n",
      "=============== BACKFILL# MERGE => OPTIMIZE => MERGE / 2024-10-05 ===============\n",
      "Versions for each run are: {'2024-10-04': 4, '2024-10-05': 6}\n",
      "Last merge version was 7\n",
      "Version to restore is 5\n",
      "Expected: restore to the OPTIMIZE version created after 2024-10-04\n",
      "=============== BACKFILL# MERGE => MERGE / 2024-10-03 ===============\n",
      "Versions for each run are: {'2024-10-02': 2, '2024-10-03': 3}\n",
      "Last merge version was 7\n",
      "Version to restore is 2\n",
      "Expected: restore to version from 2024-10-02\n",
      "=============== # NEW INSERT / 2024-10-07 ===============\n",
      "Versions for each run are: {'2024-10-06': 7}\n",
      "Last merge version was 7\n",
      "Nothing to restore: previous job's version 7 vs. last merge version 7\n",
      "Version to restore is None\n",
      "Expected: normal run\n"
     ]
    }
   ],
   "source": [
    "spark_session.sql('DESCRIBE HISTORY default.numbers_letters').select('version', 'timestamp', 'operation', 'userMetadata').show(truncate=False)\n",
    "spark_session.sql('SELECT job_version, delta_table_version FROM versions ORDER BY job_version ASC').show(truncate=False)\n",
    "\n",
    "scenarios = [\n",
    "    ('2024-10-01', None, '# FIRST MERGE', 'Expected: TRUNCATE TABLE'),\n",
    "    ('2024-10-05', '2024-10-04', 'BACKFILL# MERGE => OPTIMIZE => MERGE', 'Expected: restore to the OPTIMIZE version created after 2024-10-04'),\n",
    "    ('2024-10-03', '2024-10-02', 'BACKFILL# MERGE => MERGE', 'Expected: restore to version from 2024-10-02'),\n",
    "    ('2024-10-07', '2024-10-06', '# NEW INSERT', 'Expected: normal run'),\n",
    "]\n",
    "\n",
    "for currently_processed_version, previous_execution_date, label, expected_behavior in scenarios:\n",
    "    print(f'=============== {label} / {currently_processed_version} ===============')\n",
    "    versions = {}\n",
    "    job_with_tables = spark_session.sql(f'''SELECT job_version, delta_table_version FROM versions WHERE \n",
    "                        job_version IN (\"{currently_processed_version}\", \"{previous_execution_date}\") ORDER BY job_version ASC''').collect()\n",
    "    for row in job_with_tables:\n",
    "        versions[row.job_version] = row.delta_table_version\n",
    "    \n",
    "    print(f'Versions for each run are: {versions}')\n",
    "    \n",
    "    versions_history = spark_session.sql('DESCRIBE HISTORY default.numbers_letters')\n",
    "    versions_history.cache()\n",
    "    \n",
    "    last_merge_version = (versions_history.filter('operation = \"MERGE\"')\n",
    "     .selectExpr('MAX(version) AS last_version').collect()[0].last_version)\n",
    "    print(f'Last merge version was {last_merge_version}')\n",
    "\n",
    "    maybe_previous_job_version = spark_session.sql(f'SELECT delta_table_version FROM versions WHERE job_version = \"{previous_execution_date}\"').collect()\n",
    "    previous_job_version = None\n",
    "    version_to_restore = None\n",
    "    if maybe_previous_job_version:\n",
    "      previous_job_version = maybe_previous_job_version[0].delta_table_version\n",
    "      if previous_job_version == last_merge_version:\n",
    "       print(f\"Nothing to restore: previous job's version {previous_job_version} vs. last merge version {last_merge_version}\")\n",
    "      else:\n",
    "       current_run_version = spark_session.sql(f'SELECT delta_table_version FROM versions WHERE job_version = \"{currently_processed_version}\"').collect()[0].delta_table_version\n",
    "       version_to_restore = current_run_version - 1 \n",
    "       #(DeltaTable.forName(spark_session, 'devices').restoreToVersion(previous_job_version ))\n",
    "    else:\n",
    "       print('Missing previous version, truncating the table')\n",
    "\n",
    "    versions_history.unpersist()\n",
    "    \n",
    "    print(f'Version to restore is {version_to_restore}')\n",
    "    print(expected_behavior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4d879c-f92c-4492-980f-4b317a31405e",
   "metadata": {},
   "source": [
    "## VACUUM\n",
    "\n",
    "Let's execute a `VACUUM` operation to remove files kept by the `OPTIMIZE` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60102429-9926-497e-befc-8e5195058048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m_delta_log\u001b[0m/\n",
      "part-00000-15b8583b-52e9-4f99-9ae7-4858ecb79bcb-c000.snappy.parquet\n",
      "part-00000-332c034f-8129-4738-99e7-20fc7d7aa4fd-c000.snappy.parquet\n",
      "part-00000-816ce2fe-2510-4c96-9722-39791d926105-c000.snappy.parquet\n",
      "part-00000-85d26a94-1d53-46b9-80e3-f137cd3d95cb-c000.snappy.parquet\n",
      "part-00000-89f1cef8-8222-48b2-9f38-e1a2e3416f4d-c000.snappy.parquet\n",
      "part-00000-d8644b4d-bafa-4002-82e0-dd7189f91102-c000.snappy.parquet\n",
      "part-00000-f532b1f5-f052-4f4b-bb55-e2574d6bf316-c000.snappy.parquet\n",
      "part-00002-27c58b71-6b5e-45a0-a2d0-32eca4141cea-c000.snappy.parquet\n",
      "part-00005-22054b02-33a3-498a-978b-8cf110103684-c000.snappy.parquet\n",
      "part-00007-1fd4a6e0-93f7-4d45-99b4-dd6f16149508-c000.snappy.parquet\n",
      "part-00007-633dcb64-f53b-4590-8d3f-ede3457b71ec-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    " ls ./spark-warehouse/numbers_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "459c4e1d-7cc2-4825-930b-62f764c59e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 9 files and directories in a total of 1 directories.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "spark_session.conf.set('spark.databricks.delta.retentionDurationCheck.enabled', False)\n",
    "numbers_letters_delta_table = DeltaTable.forName(spark_session, 'numbers_letters')\n",
    "numbers_letters_delta_table.vacuum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36f54df8-6ba7-4a03-816e-35593151672b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m_delta_log\u001b[0m/\n",
      "part-00000-89f1cef8-8222-48b2-9f38-e1a2e3416f4d-c000.snappy.parquet\n",
      "part-00007-1fd4a6e0-93f7-4d45-99b4-dd6f16149508-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    " ls ./spark-warehouse/numbers_letters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cead83-89d8-4f48-8d59-09309fbaf637",
   "metadata": {},
   "source": [
    "As you can see, 9 files are gone. Let's see if we can time-travel to each of the versions from the state table. \n",
    "In this example I'm using the _time travel_ feature but in the MERGER logic presented before, I refered to the `restore` command. The reason I'm time travelling here is that the table doesn't get materialized, i.e. there is no restore commit added, thus no new versions. Running things in isolation is an important property for any testing approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31d13707-166b-43ff-8808-6f8422949591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time travel for 2024-10-01 and Delta Lake table 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/23 06:14:34 ERROR Executor: Exception in task 0.0 in stage 436.0 (TID 13259)\n",
      "org.apache.spark.SparkFileNotFoundException: File file:/home/bartosz/workspace/delta-lake-playground/017_merger/spark-warehouse/numbers_letters/part-00005-22054b02-33a3-498a-978b-8cf110103684-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/11/23 06:14:34 WARN TaskSetManager: Lost task 0.0 in stage 436.0 (TID 13259) (192.168.1.55 executor driver): org.apache.spark.SparkFileNotFoundException: File file:/home/bartosz/workspace/delta-lake-playground/017_merger/spark-warehouse/numbers_letters/part-00005-22054b02-33a3-498a-978b-8cf110103684-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/11/23 06:14:34 ERROR TaskSetManager: Task 0 in stage 436.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Couldn't time-travel for 1\n",
      "Testing time travel for 2024-10-02 and Delta Lake table 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/23 06:14:36 ERROR Executor: Exception in task 0.0 in stage 446.0 (TID 13367)\n",
      "org.apache.spark.SparkFileNotFoundException: File file:/home/bartosz/workspace/delta-lake-playground/017_merger/spark-warehouse/numbers_letters/part-00000-15b8583b-52e9-4f99-9ae7-4858ecb79bcb-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/11/23 06:14:36 WARN TaskSetManager: Lost task 0.0 in stage 446.0 (TID 13367) (192.168.1.55 executor driver): org.apache.spark.SparkFileNotFoundException: File file:/home/bartosz/workspace/delta-lake-playground/017_merger/spark-warehouse/numbers_letters/part-00000-15b8583b-52e9-4f99-9ae7-4858ecb79bcb-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/11/23 06:14:36 ERROR TaskSetManager: Task 0 in stage 446.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Couldn't time-travel for 2\n",
      "Testing time travel for 2024-10-03 and Delta Lake table 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/23 06:14:38 ERROR Executor: Exception in task 0.0 in stage 456.0 (TID 13477)\n",
      "org.apache.spark.SparkFileNotFoundException: File file:/home/bartosz/workspace/delta-lake-playground/017_merger/spark-warehouse/numbers_letters/part-00000-d8644b4d-bafa-4002-82e0-dd7189f91102-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/11/23 06:14:38 WARN TaskSetManager: Lost task 0.0 in stage 456.0 (TID 13477) (192.168.1.55 executor driver): org.apache.spark.SparkFileNotFoundException: File file:/home/bartosz/workspace/delta-lake-playground/017_merger/spark-warehouse/numbers_letters/part-00000-d8644b4d-bafa-4002-82e0-dd7189f91102-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/11/23 06:14:38 ERROR TaskSetManager: Task 0 in stage 456.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Couldn't time-travel for 3\n",
      "Testing time travel for 2024-10-04 and Delta Lake table 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/23 06:14:39 ERROR Executor: Exception in task 0.0 in stage 466.0 (TID 13589)\n",
      "org.apache.spark.SparkFileNotFoundException: File file:/home/bartosz/workspace/delta-lake-playground/017_merger/spark-warehouse/numbers_letters/part-00000-f532b1f5-f052-4f4b-bb55-e2574d6bf316-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/11/23 06:14:39 WARN TaskSetManager: Lost task 0.0 in stage 466.0 (TID 13589) (192.168.1.55 executor driver): org.apache.spark.SparkFileNotFoundException: File file:/home/bartosz/workspace/delta-lake-playground/017_merger/spark-warehouse/numbers_letters/part-00000-f532b1f5-f052-4f4b-bb55-e2574d6bf316-c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/11/23 06:14:39 ERROR TaskSetManager: Task 0 in stage 466.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Couldn't time-travel for 4\n",
      "Testing time travel for 2024-10-05 and Delta Lake table 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|number|letter|\n",
      "+------+------+\n",
      "|1     |AAAA  |\n",
      "|2     |b     |\n",
      "|3     |c     |\n",
      "|4     |d     |\n",
      "|5     |e     |\n",
      "|6     |f     |\n",
      "|7     |g     |\n",
      "|8     |h     |\n",
      "|9     |i     |\n",
      "+------+------+\n",
      "\n",
      "✅ Correctly time-travelled for 6\n",
      "Testing time travel for 2024-10-06 and Delta Lake table 7\n",
      "+------+------+\n",
      "|number|letter|\n",
      "+------+------+\n",
      "|1     |AAAA  |\n",
      "|2     |b     |\n",
      "|3     |c     |\n",
      "|4     |d     |\n",
      "|5     |e     |\n",
      "|6     |f     |\n",
      "|7     |g     |\n",
      "|8     |h     |\n",
      "|9     |i     |\n",
      "|10    |j     |\n",
      "+------+------+\n",
      "\n",
      "✅ Correctly time-travelled for 7\n"
     ]
    }
   ],
   "source": [
    "versions_history = spark_session.sql('SELECT job_version, delta_table_version FROM versions ORDER BY job_version ASC').collect()\n",
    "\n",
    "for version_history in versions_history:\n",
    "    print(f'Testing time travel for {version_history.job_version} and Delta Lake table {version_history.delta_table_version}')\n",
    "    try:\n",
    "        (spark_session.read.format(\"delta\").option(\"versionAsOf\", version_history.delta_table_version)\n",
    "        .load(\"./spark-warehouse/numbers_letters\").show(truncate=False))\n",
    "        print(f'✅ Correctly time-travelled for {version_history.delta_table_version}')\n",
    "    except:\n",
    "        print(f\"❌ Couldn't time-travel for {version_history.delta_table_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dad514-94af-4fbe-ad1c-536a9e0a39bd",
   "metadata": {},
   "source": [
    "As you can notice, two last versions which are the versions after the `OPTIMIZE` operation are only available for time travel / restore."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
