{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "903cf788-981e-49a3-ac79-3dc8a66802ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/08/23 06:18:15 WARN Utils: Your hostname, bartosz, resolves to a loopback address: 127.0.1.1; using 192.168.1.55 instead (on interface wlp0s20f3)\n",
      "25/08/23 06:18:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/bartosz/.venvs/delta_spark_4/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/bartosz/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /home/bartosz/.ivy2.5.2/jars\n",
      "io.delta#delta-spark_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c76d94bb-d1d5-402c-bf37-a62a848bc804;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.13;4.0.0 in central\n",
      "\tfound io.delta#delta-storage;4.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.13.1 in local-m2-cache\n",
      ":: resolution report :: resolve 228ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.13;4.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;4.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.13.1 from local-m2-cache in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c76d94bb-d1d5-402c-bf37-a62a848bc804\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/6ms)\n",
      "25/08/23 06:18:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from delta import configure_spark_with_delta_pip, DeltaTable\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_session = (configure_spark_with_delta_pip(SparkSession.builder.master(\"local[*]\")\n",
    "                                                        .config(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "                                                .config(\"spark.sql.extensions\",\n",
    "                                                        \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "                                                .config(\"spark.sql.catalog.spark_catalog\",\n",
    "                                                        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "                                                ).getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71880432-077b-4bed-b499-6ef8f8cd5bcc",
   "metadata": {},
   "source": [
    "**Create input tables first**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0622a7c3-7c7a-4fff-87eb-7723ea58f063",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf ./spark-warehouse && rm -rf ./metastore_db/ && rm -rf ./checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faec8d86-aa35-45ae-b055-0094f6406dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/23 06:18:25 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "25/08/23 06:18:25 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore bartosz@127.0.1.1\n",
      "25/08/23 06:18:25 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException\n",
      "25/08/23 06:18:28 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`default`.`table_1` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "25/08/23 06:18:28 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "25/08/23 06:18:28 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = 'table_1'\n",
    "print(f'Creating {table}')\n",
    "spark_session.sql(f'DROP TABLE IF EXISTS `default`.`{table}`')\n",
    "spark_session.sql(f'''\n",
    "  CREATE TABLE `default`.`{table}` (\n",
    "     number INT NOT NULL,\n",
    "     letter STRING NOT NULL\n",
    "  ) \n",
    "  USING DELTA \n",
    "  TBLPROPERTIES (delta.enableChangeDataFeed = true) \n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984a15e0-f2cd-4b42-ac07-7994d7b04c1d",
   "metadata": {},
   "source": [
    "# Change Data Feed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56a0f87-ba61-4a99-8883-2022ff3fb1df",
   "metadata": {},
   "source": [
    "Change Data Feed (CDF) has a limited usage in the context of dual writes problem. It only works when you try to write the same dataset into a Delta Lake table and a streaming broker. Assuming that _streaming broker_ is an abstract concept that can be substituted by everything _streamable_, CDF eliminates the need of dual writes as the streaming consumers can process the Delta Lake table directly.\n",
    "\n",
    "Let's see this in action by inserting some rows to our table with the CDF enabled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37de6bcc-ba26-47b9-9be8-1dc81bcbc18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/23 06:19:10 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|number|letter|\n",
      "+------+------+\n",
      "|     2|     b|\n",
      "|     1|     a|\n",
      "|     3|     c|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import Row\n",
    "\n",
    "data_to_write = spark_session.createDataFrame([Row(number=1, letter='a'), Row(number=2, letter='b'), Row(number=3, letter='c')])\n",
    "data_to_write.write.format('delta').mode('overwrite').insertInto(table)\n",
    "\n",
    "spark_session.read.table(table).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72fe8ec-cadb-439a-b5fb-eebf63d26231",
   "metadata": {},
   "source": [
    "Let's stream the changes now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cf0bcaa-5230-41d4-bfff-ebb19c605045",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/23 06:19:20 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------+------+------------+---------------+--------------------+\n",
      "|number|letter|_change_type|_commit_version|   _commit_timestamp|\n",
      "+------+------+------------+---------------+--------------------+\n",
      "|     1|     a|      insert|              1|2025-08-23 06:19:...|\n",
      "|     2|     b|      insert|              1|2025-08-23 06:19:...|\n",
      "|     3|     c|      insert|              1|2025-08-23 06:19:...|\n",
      "+------+------+------------+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "changed_rows_stream = (spark_session.readStream.format('delta')\n",
    "  .option('readChangeFeed', 'true')\n",
    "  .option('startingVersion', 0)\n",
    "  .table(table)\n",
    "  .writeStream.option('checkpointLocation', './checkpoint').format(\"console\").trigger(availableNow=True))\n",
    "\n",
    "changed_rows_stream.start().awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bae0c8f-d02c-4e3d-b58f-738f599df4b9",
   "metadata": {},
   "source": [
    "Let's do some other changes in the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9472aa52-8429-436f-a187-4fb39aff7459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/23 06:19:30 WARN MapPartitionsRDD: RDD 71 was locally checkpointed, its lineage has been truncated and cannot be recomputed after unpersisting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|number|letter|\n",
      "+------+------+\n",
      "|     1|     A|\n",
      "|     2|     B|\n",
      "|     4|     D|\n",
      "|     3|     c|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_to_write = spark_session.createDataFrame([Row(number=1, letter='A'), Row(number=2, letter='B'), Row(number=4, letter='D')])\n",
    "(DeltaTable.forName(spark_session, table).alias('base_table')\n",
    " .merge(data_to_write.alias('new_table'), 'base_table.number = new_table.number')\n",
    " .whenMatchedUpdateAll().whenNotMatchedInsertAll().execute())\n",
    "spark_session.read.table(table).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6941cd2b-6c90-45af-9fe5-d6d65b02adbb",
   "metadata": {},
   "source": [
    "If we run the stream again, we should be able to get the updated and inserted records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f890648-a704-4cb8-9b1b-fd6f949ebd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/23 06:19:59 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+------+------+----------------+---------------+--------------------+\n",
      "|number|letter|    _change_type|_commit_version|   _commit_timestamp|\n",
      "+------+------+----------------+---------------+--------------------+\n",
      "|     1|     a| update_preimage|              2|2025-08-23 06:19:...|\n",
      "|     1|     A|update_postimage|              2|2025-08-23 06:19:...|\n",
      "|     2|     b| update_preimage|              2|2025-08-23 06:19:...|\n",
      "|     2|     B|update_postimage|              2|2025-08-23 06:19:...|\n",
      "|     4|     D|          insert|              2|2025-08-23 06:19:...|\n",
      "+------+------+----------------+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "changed_rows_stream.start().awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
