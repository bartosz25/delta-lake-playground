{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "903cf788-981e-49a3-ac79-3dc8a66802ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/08/23 06:20:07 WARN Utils: Your hostname, bartosz, resolves to a loopback address: 127.0.1.1; using 192.168.1.55 instead (on interface wlp0s20f3)\n",
      "25/08/23 06:20:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/bartosz/.venvs/delta_spark_4/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/bartosz/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /home/bartosz/.ivy2.5.2/jars\n",
      "io.delta#delta-spark_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-7382ed67-4f7d-4d0e-9078-0d808d0331f1;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.13;4.0.0 in central\n",
      "\tfound io.delta#delta-storage;4.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.13.1 in local-m2-cache\n",
      ":: resolution report :: resolve 185ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.13;4.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;4.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.13.1 from local-m2-cache in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-7382ed67-4f7d-4d0e-9078-0d808d0331f1\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/4ms)\n",
      "25/08/23 06:20:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/23 06:20:09 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from delta import configure_spark_with_delta_pip, DeltaTable\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_session = (configure_spark_with_delta_pip(SparkSession.builder.master(\"local[*]\")\n",
    "                                                        .config(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "                                                .config(\"spark.sql.extensions\",\n",
    "                                                        \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "                                                .config(\"spark.sql.catalog.spark_catalog\",\n",
    "                                                        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "                                                ).getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71880432-077b-4bed-b499-6ef8f8cd5bcc",
   "metadata": {},
   "source": [
    "**Create input tables first**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0622a7c3-7c7a-4fff-87eb-7723ea58f063",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf ./spark-warehouse && rm -rf ./metastore_db/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faec8d86-aa35-45ae-b055-0094f6406dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/23 06:20:21 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "25/08/23 06:20:21 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore bartosz@127.0.1.1\n",
      "25/08/23 06:20:21 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException\n",
      "25/08/23 06:20:24 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`default`.`table_1` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "25/08/23 06:20:24 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "25/08/23 06:20:24 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/23 06:20:25 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`default`.`table_2` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    }
   ],
   "source": [
    "tables = ['table_1', 'table_2']\n",
    "for table in tables:\n",
    "    print(f'Creating {table}')\n",
    "    spark_session.sql(f'DROP TABLE IF EXISTS `default`.`{table}`')\n",
    "    spark_session.sql(f'''\n",
    "              CREATE TABLE `default`.`{table}` (\n",
    "                 number INT NOT NULL,\n",
    "                letter STRING NOT NULL\n",
    "              ) USING DELTA\n",
    "            ''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984a15e0-f2cd-4b42-ac07-7994d7b04c1d",
   "metadata": {},
   "source": [
    "# Transaction compensation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2a593c-30f0-494a-b15d-a205984ecffc",
   "metadata": {},
   "source": [
    "Let's see how the transaction compensation works by adding rows to the first of two tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9a7d3f3-4720-4287-8609-8b73d3d4259d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/23 06:20:50 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|number|letter|\n",
      "+------+------+\n",
      "|     3|     c|\n",
      "|     1|     a|\n",
      "|     2|     b|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import Row\n",
    "\n",
    "data_to_write = spark_session.createDataFrame([Row(number=1, letter='a'), Row(number=2, letter='b'), Row(number=3, letter='c')])\n",
    "\n",
    "data_to_write.write.format('delta').mode('overwrite').insertInto('table_1')\n",
    "spark_session.read.table('table_1').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b0b8b4-b381-40cb-bf77-afc90d8bad76",
   "metadata": {},
   "source": [
    "Let's now suppose we want to write the same dataset to the *table_2* but the writer fails with some random exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "844bca95-eb33-456e-baf1-74f19e9913e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Some error occurred",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mSome error occurred\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Some error occurred"
     ]
    }
   ],
   "source": [
    "raise RuntimeError('Some error occurred')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c73be97-fe4d-4efc-b2e4-49666dee8337",
   "metadata": {},
   "source": [
    "If we need to apply the **`Transaction compensation`** with Delta Lake, we need to:\n",
    "* find the previous valid version of the correctly written tables (table_1 in our case)\n",
    "* call the `RESTORE` function to rollback the table to this most recent version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ccc3501-5e88-425b-a47c-9f5ae7465529",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`default`.`table_1` will be restored to 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[table_size_after_restore: bigint, num_of_files_after_restore: bigint, num_removed_files: bigint, num_restored_files: bigint, removed_files_size: bigint, restored_files_size: bigint]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### in your production code you can keep a list of the successfuly written tables; for our simplistic use case it would be overkill\n",
    "table_to_restore = '`default`.`table_1`'\n",
    "spark_session.sql(f'DESCRIBE HISTORY {table_to_restore}').createOrReplaceTempView('history')\n",
    "version_to_restore_df = spark_session.sql(f'''\n",
    "SELECT MIN(version) AS restored_version FROM (\n",
    "    SELECT * FROM history v ORDER BY version DESC LIMIT 2\n",
    ") vv\n",
    "''').collect()\n",
    "version_to_restore = version_to_restore_df[0].restored_version\n",
    "\n",
    "print(f'{table_to_restore} will be restored to {version_to_restore}')\n",
    "spark_session.sql(f'RESTORE {table_to_restore} TO VERSION AS OF {version_to_restore}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f57072d-437c-4bfc-9b99-d9baf8b2d3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|number|letter|\n",
      "+------+------+\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_session.read.table('table_1').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56a0f87-ba61-4a99-8883-2022ff3fb1df",
   "metadata": {},
   "source": [
    "# Wrap-up\n",
    "Let's wrap the *Transaction compensation* into a more realistic code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7b527bd-bcf2-411e-8512-9969643e424e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_table_to_previous_version(table_to_restore: str):\n",
    "    spark_session.sql(f'DESCRIBE HISTORY {table_to_restore}').createOrReplaceTempView('history')\n",
    "    version_to_restore_df = spark_session.sql(f'''\n",
    "    SELECT MIN(version) AS restored_version FROM (\n",
    "        SELECT * FROM history v ORDER BY version DESC LIMIT 2\n",
    "    ) vv\n",
    "    ''').collect()\n",
    "    version_to_restore = version_to_restore_df[0].restored_version\n",
    "    \n",
    "    print(f'{table_to_restore} will be restored to {version_to_restore}')\n",
    "    spark_session.sql(f'RESTORE {table_to_restore} TO VERSION AS OF {version_to_restore}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37de6bcc-ba26-47b9-9be8-1dc81bcbc18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table_1 will be restored to 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|number|letter|\n",
      "+------+------+\n",
      "+------+------+\n",
      "\n",
      "+------+------+\n",
      "|number|letter|\n",
      "+------+------+\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import Row\n",
    "\n",
    "data_to_write = spark_session.createDataFrame([Row(number=1, letter='a'), Row(number=2, letter='b'), Row(number=3, letter='c')])\n",
    "\n",
    "successfully_written_tables = []\n",
    "\n",
    "def write_data(table: str):\n",
    "    try:\n",
    "        if table == 'table_2':\n",
    "            raise RuntimeError('Some random error')\n",
    "        data_to_write.write.format('delta').mode('overwrite').insertInto(table)\n",
    "        successfully_written_tables.append(table)\n",
    "    except:\n",
    "        for table_to_restore in successfully_written_tables:\n",
    "            restore_table_to_previous_version(table_to_restore)\n",
    "\n",
    "write_data('table_1')\n",
    "write_data('table_2')\n",
    "\n",
    "spark_session.read.table('table_1').show()\n",
    "spark_session.read.table('table_2').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
