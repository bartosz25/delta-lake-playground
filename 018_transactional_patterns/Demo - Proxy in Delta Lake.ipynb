{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "903cf788-981e-49a3-ac79-3dc8a66802ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/08/23 06:23:04 WARN Utils: Your hostname, bartosz, resolves to a loopback address: 127.0.1.1; using 192.168.1.55 instead (on interface wlp0s20f3)\n",
      "25/08/23 06:23:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/bartosz/.venvs/delta_spark_4/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/bartosz/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /home/bartosz/.ivy2.5.2/jars\n",
      "io.delta#delta-spark_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-5c595551-a2a8-4c3a-bfb6-babc24e8a34d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.13;4.0.0 in central\n",
      "\tfound io.delta#delta-storage;4.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.13.1 in local-m2-cache\n",
      ":: resolution report :: resolve 167ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.13;4.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;4.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.13.1 from local-m2-cache in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-5c595551-a2a8-4c3a-bfb6-babc24e8a34d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/7ms)\n",
      "25/08/23 06:23:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/23 06:23:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/08/23 06:23:06 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from delta import configure_spark_with_delta_pip, DeltaTable\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_session = (configure_spark_with_delta_pip(SparkSession.builder.master(\"local[*]\")\n",
    "                                                        .config(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "                                                .config(\"spark.sql.extensions\",\n",
    "                                                        \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "                                                .config(\"spark.sql.catalog.spark_catalog\",\n",
    "                                                        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "                                                ).getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71880432-077b-4bed-b499-6ef8f8cd5bcc",
   "metadata": {},
   "source": [
    "**Create input tables first**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0622a7c3-7c7a-4fff-87eb-7723ea58f063",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf ./spark-warehouse && rm -rf ./metastore_db/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faec8d86-aa35-45ae-b055-0094f6406dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/23 06:23:14 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "25/08/23 06:23:14 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore bartosz@127.0.1.1\n",
      "25/08/23 06:23:14 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException\n",
      "25/08/23 06:23:17 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`default`.`table_1` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "25/08/23 06:23:17 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "25/08/23 06:23:17 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/23 06:23:18 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`default`.`table_2` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    }
   ],
   "source": [
    "tables = ['table_1', 'table_2']\n",
    "for table in tables:\n",
    "    print(f'Creating {table}')\n",
    "    spark_session.sql(f'DROP TABLE IF EXISTS `default`.`{table}`')\n",
    "    spark_session.sql(f'''\n",
    "              CREATE TABLE `default`.`{table}` (\n",
    "                 number INT NOT NULL,\n",
    "                letter STRING NOT NULL\n",
    "              ) USING DELTA\n",
    "            ''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984a15e0-f2cd-4b42-ac07-7994d7b04c1d",
   "metadata": {},
   "source": [
    "# Reverse proxy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5185e3e-d05a-4027-b163-5bb92ef7a9dc",
   "metadata": {},
   "source": [
    "Let's start with writing some rows to the first of two tables involves in the dual write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9a7d3f3-4720-4287-8609-8b73d3d4259d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/23 06:23:25 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|number|letter|\n",
      "+------+------+\n",
      "|     3|     c|\n",
      "|     1|     a|\n",
      "|     2|     b|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import Row\n",
    "\n",
    "data_to_write = spark_session.createDataFrame([Row(number=1, letter='a'), Row(number=2, letter='b'), Row(number=3, letter='c')])\n",
    "\n",
    "data_to_write.write.format('delta').mode('overwrite').insertInto('table_1')\n",
    "spark_session.read.table('table_1').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b0b8b4-b381-40cb-bf77-afc90d8bad76",
   "metadata": {},
   "source": [
    "Let's now suppose we want to write the same dataset to the *table_2* but the writer fails with some random exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "844bca95-eb33-456e-baf1-74f19e9913e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Some error occurred",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mSome error occurred\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Some error occurred"
     ]
    }
   ],
   "source": [
    "raise RuntimeError('Some error occurred')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c73be97-fe4d-4efc-b2e4-49666dee8337",
   "metadata": {},
   "source": [
    "If we need to apply the **`Proxy`** with Delta Lake, we need to create the views with most recent commit versions for both tables only when the writes to both tables succeeded. As this approach requires some extra table to track the last successful commits per table, let's move directly to the wrap-up part where you will find a more complete example.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56a0f87-ba61-4a99-8883-2022ff3fb1df",
   "metadata": {},
   "source": [
    "# Wrap-up\n",
    "Let's wrap the *Proxy* into a more realistic code. We create here two tables involved in the dual write and a mapping table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9116979a-ff19-4460-8267-6704ae16d36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/23 06:27:46 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`default`.`table_3` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/23 06:27:47 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`default`.`table_4` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "25/08/23 06:27:47 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`default`.`last_versions_per_table` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables = ['table_3', 'table_4']\n",
    "for table in tables:\n",
    "    print(f'Creating {table}')\n",
    "    spark_session.sql(f'DROP TABLE IF EXISTS `default`.`{table}`')\n",
    "    spark_session.sql(f'''\n",
    "              CREATE TABLE `default`.`{table}` (\n",
    "                 number INT NOT NULL,\n",
    "                letter STRING NOT NULL\n",
    "              ) USING DELTA\n",
    "            ''')\n",
    "\n",
    "spark_session.sql(f'DROP TABLE IF EXISTS `default`.`last_versions_per_table`')\n",
    "spark_session.sql(f'''\n",
    "          CREATE TABLE `default`.`last_versions_per_table` (\n",
    "             version INT NOT NULL,\n",
    "             table_name STRING NOT NULL\n",
    "          ) USING DELTA\n",
    "        ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7b527bd-bcf2-411e-8512-9969643e424e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_version(table_name: str) -> int:\n",
    "    spark_session.sql(f'DESCRIBE HISTORY {table_name}').createOrReplaceTempView('history')\n",
    "    versions_df = spark_session.sql(f'SELECT MAX(version) AS version FROM history ORDER BY version DESC LIMIT 2').collect()\n",
    "    return versions_df[0].version\n",
    "\n",
    "\n",
    "def get_all_versions_from_the_last_versions_per_table():\n",
    "    versions = spark_session.sql('SELECT table_name, version FROM last_versions_per_table').collect()\n",
    "    versions_dict = {row.table_name: row.version for row in versions}\n",
    "    return versions_dict\n",
    "\n",
    "def update_last_versions_in_last_versions_per_table(tables_with_versions: list[(str, int)]):\n",
    "    print(tables_with_versions)\n",
    "    updates = [Row(table_name=table_version[0], version=table_version[1]) for table_version in tables_with_versions.items()]\n",
    "    update_to_write = spark_session.createDataFrame(updates)\n",
    "    (DeltaTable.forName(spark_session, 'last_versions_per_table').alias('base_table')\n",
    "     .merge(update_to_write.alias('new_table'), 'base_table.table_name = new_table.table_name')\n",
    "     .whenMatchedUpdateAll().whenNotMatchedInsertAll().execute())\n",
    "\n",
    "def create_view(table_for_proxy: str, tables_with_versions: dict[str, int]):\n",
    "    version_to_use = 0\n",
    "    if table_for_proxy in tables_with_versions:\n",
    "        version_to_use = tables_with_versions[table_for_proxy]\n",
    "    view_name = f'view_{table_for_proxy}'\n",
    "    spark_session.sql(f'DROP VIEW IF EXISTS {view_name}')\n",
    "    print(f'Creating view {view_name} from table {table_for_proxy} and version {version_to_use}')\n",
    "    spark_session.sql(f'''CREATE VIEW {view_name} AS \n",
    "        (SELECT * FROM {table_for_proxy} VERSION AS OF {version_to_use})''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37de6bcc-ba26-47b9-9be8-1dc81bcbc18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error!\n",
      "Some random error\n",
      "result_1=True // result_2=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating view view_table_3 from table table_3 and version 0\n",
      "Creating view view_table_4 from table table_4 and version 0\n",
      "{}\n",
      "+------+------+\n",
      "|number|letter|\n",
      "+------+------+\n",
      "+------+------+\n",
      "\n",
      "+------+------+\n",
      "|number|letter|\n",
      "+------+------+\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import Row\n",
    "\n",
    "def demo(should_fail: bool):\n",
    "    data_to_write = spark_session.createDataFrame([Row(number=1, letter='a'), Row(number=2, letter='b'), Row(number=3, letter='c')])\n",
    "    \n",
    "    tables_with_versions = {}\n",
    "    \n",
    "    def write_data(table: str) -> bool:\n",
    "        try:\n",
    "            if should_fail and table == 'table_4':\n",
    "                print('Error!')\n",
    "                raise RuntimeError('Some random error')\n",
    "            data_to_write.write.format('delta').mode('overwrite').insertInto(table)\n",
    "            tables_with_versions[table] = get_last_version(table)\n",
    "            return True\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "            return False\n",
    "    \n",
    "    result_1 = write_data('table_3')\n",
    "    result_2 = write_data('table_4')\n",
    "    print(f'result_1={result_1} // result_2={result_2}')\n",
    "    if result_1 and result_2:\n",
    "        print(tables_with_versions)\n",
    "        update_last_versions_in_last_versions_per_table(tables_with_versions)\n",
    "        all_versions = tables_with_versions\n",
    "    else:\n",
    "        all_versions = get_all_versions_from_the_last_versions_per_table()\n",
    "    \n",
    "    create_view('table_3', all_versions)\n",
    "    create_view('table_4', all_versions)\n",
    "    print(all_versions)\n",
    "    spark_session.read.table('view_table_3').show()\n",
    "    spark_session.read.table('view_table_4').show()\n",
    "\n",
    "demo(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72fe8ec-cadb-439a-b5fb-eebf63d26231",
   "metadata": {},
   "source": [
    "Let's see now what happens if we have some committed entries in the last_versions_per_table table. Let's create a successful write first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cf0bcaa-5230-41d4-bfff-ebb19c605045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result_1=True // result_2=True\n",
      "{'table_3': 2, 'table_4': 1}\n",
      "{'table_3': 2, 'table_4': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/23 06:29:18 WARN MapPartitionsRDD: RDD 281 was locally checkpointed, its lineage has been truncated and cannot be recomputed after unpersisting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating view view_table_3 from table table_3 and version 2\n",
      "Creating view view_table_4 from table table_4 and version 1\n",
      "{'table_3': 2, 'table_4': 1}\n",
      "+------+------+\n",
      "|number|letter|\n",
      "+------+------+\n",
      "|     1|     a|\n",
      "|     2|     b|\n",
      "|     3|     c|\n",
      "+------+------+\n",
      "\n",
      "+------+------+\n",
      "|number|letter|\n",
      "+------+------+\n",
      "|     3|     c|\n",
      "|     2|     b|\n",
      "|     1|     a|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demo(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bae0c8f-d02c-4e3d-b58f-738f599df4b9",
   "metadata": {},
   "source": [
    "And now, let's suppose the dual write fails. The view shouldn't be upgraded to the partially committed tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9472aa52-8429-436f-a187-4fb39aff7459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error!\n",
      "Some random error\n",
      "result_1=True // result_2=False\n",
      "Creating view view_table_3 from table table_3 and version 2\n",
      "Creating view view_table_4 from table table_4 and version 1\n",
      "{'table_4': 1, 'table_3': 2}\n",
      "+------+------+\n",
      "|number|letter|\n",
      "+------+------+\n",
      "|     1|     a|\n",
      "|     2|     b|\n",
      "|     3|     c|\n",
      "+------+------+\n",
      "\n",
      "+------+------+\n",
      "|number|letter|\n",
      "+------+------+\n",
      "|     3|     c|\n",
      "|     2|     b|\n",
      "|     1|     a|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demo(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
